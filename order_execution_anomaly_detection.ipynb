{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why PySpark\n",
    "\n",
    "PySpark is an interface to Apache Spark in Python. It allows you to analyze data interactively in a distributed environment. If your data volume is large, such as transaction detail and execution data, you need to find a stable storage to store these daily data, and also you need to have a cheap and powerful computing tool to process and analyze this data. hadoop HDFS and Jupyter with pyspark is a good choice for you. PySpark provides some features such as Spark SQL, DataFrame and MLlib (machine learning), which is is really BA friendly.\n",
    "\n",
    "## Data\n",
    "\n",
    "The daily order execution data is stored in daily partition in hdfs as parquet format files. As a data scientist, we can extract some and save in our user dedicated folder as csv format file. The data schema is as follws: \n",
    "```\n",
    "Field name        Field type\n",
    "Id                String\n",
    "Symbol            String\n",
    "LastMkt           String\n",
    "Datetime          String\n",
    "Brokerid          String\n",
    "Traderid          String\n",
    "Clientid          String\n",
    "Account           String\n",
    "Country           String\n",
    "effectiveTime     String\n",
    "expireTime        String\n",
    "timeInForce       String\n",
    "exposureDuration  String\n",
    "tradingSession    String\n",
    "tradingSessionSub String\n",
    "settlType         String\n",
    "settlDate         String\n",
    "Currency          String\n",
    "currencyFXRate    Float\n",
    "execType          String\n",
    "ExDestination     String\n",
    "trdType           String\n",
    "matchType         String\n",
    "Side              String\n",
    "OrdStatus         String\n",
    "OrdType           String\n",
    "orderQty          Int\n",
    "Price             Float\n",
    "exchangeCode      String\n",
    "refQuoteId        String\n",
    "refOrderId        String\n",
    "trdPlatform       String\n",
    "```\n",
    "Please refer to https://www.onixs.biz/fix-dictionary/4.2/fields_by_name.html for the meaning of detailed data fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Trade Execution Anomaly Detection').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Execution Data\n",
    "\n",
    "Execution.csv is located in my hdfs home folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('Execution.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-------+---------+-------+--------+--------+--------------+---------+-------+----+------+--------+-----------+--------------+-------------------+------------+----------------+---------+\n",
      "|          Id|ExDestination|LastMkt|  Account|Country|Currency|execType|leaf_exec_flag|OrdStatus|OrdType|Side|Symbol|TraderId|trdPlatform|drv_event_type|drv_cross_exec_flag|drv_1ast_mkt|account_mnemonic|region_cd|\n",
      "+------------+-------------+-------+---------+-------+--------+--------+--------------+---------+-------+----+------+--------+-----------+--------------+-------------------+------------+----------------+---------+\n",
      "|565 98267758|          QFF|   XNAs| RENTADMA|    USA|     USD|       F|             N|        1|      2|   2|    cc| be14462|     RIOQFF|   FILLED_CONF|                  N|         OTC|        00301215|      NAM|\n",
      "| 56584040534|     CITISMRT|   CDED|  AQRTEST|    USA|     USD|       F|             N|        1|      2|   1|   FLS| md42277|     RIOQFF|   FILLED_CONF|                  N|        CDED|        0614375A|      NAM|\n",
      "| 56583387595|         NSDQ|   XNMS|  MLPUSAE|    USA|     USD|       F|             Y|        1|      2|   5|   CFG| md42277|  RIOCSAGEN|   FILLED_CONF|                  N|        XNMS|        0614256D|      NAM|\n",
      "| 56533030399|     CITISMRT|   EDGX| NORDEFIX|    USA|     USD|       F|             N|        1|      2|   2|  ENOB| be14462|     RIOQFF|   FILLED_CONF|                  N|        EDGX|        CGNRDZTU|      NAM|\n",
      "| 56651329561|         XNYS|   XNYS| ADGE_ATR|    USA|     USD|       F|             Y|        1|      2|   2|  KEYS| r857654|       XSOR|   FILLED_CONF|                  N|        XNYS|        0615380I|      NAM|\n",
      "+------------+-------------+-------+---------+-------+--------+--------+--------------+---------+-------+----+------+--------+-----------+--------------+-------------------+------------+----------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "##### Feature selection\n",
    "Some columns have no obvious contribution to the classification results, we can drop them directly, and some other features have multicollinearity but we can't find the closely linear relationship directly, we can use pair plot to check, or a better way is use correlation matrix to check how closely related the features are. For latter one, we can do like this:\n",
    "```\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vector_col = \"corr_features\"\n",
    "assembler = VectorAssembler(inputCols=df.columns, outputCol=vector_col)\n",
    "df_vector = assembler.transform(df).select(vector_col)\n",
    "matrix = Correlation.corr(df_vector, vector_col)\n",
    "cor_np = matrix.collect()[0][matrix.columns[0]].toArray()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Id', 'Symbol', 'Datetime', 'effectiveTime', 'expireTime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna('Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing all necessary ML modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Dataset Into train, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.8, 0.2], 4321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_indexer = StringIndexer(inputCol=\"trdPlatform\", outputCol=\"label\", handleInvalid=\"keep\").fit(train)\n",
    "train = label_indexer.transform(train)\n",
    "test = label_indexer.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [dtype[0] for dtype in df.dtypes if dtype[1].startswith('string') and dtype[0] != 'trdPlatform']\n",
    "indexers = [StringIndexer(inputCol=column, outputCol='{0}_index'.format(column), handleInvalid=\"keep\") for column in categorical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging columns into a vector features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [dtype[0] for dtype in train.dtypes if dtype[1].startswith('double')]\n",
    "assembler = VectorAssembler(inputCols=[indexer.getOutputCol() for indexer in indexers] + numeric_columns, outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", withStd=True, withMean=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [len(assembler.getInputCols()), 120, 60, len(label_indexer.labels)+1]\n",
    "classifier = MultilayerPerceptronClassifier(labelCol='label',\n",
    "                                            featuresCol='features_scaled',\n",
    "                                            maxIter=100,\n",
    "                                            layers=layers,\n",
    "                                            blockSize=128,\n",
    "                                            seed=4321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=indexers + [assembler, scaler, classifier])\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.transform(train)\n",
    "test_preds = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_labels = train_preds.select('prediction', 'label')\n",
    "test_pred_labels = test_preds.select('prediction', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train weightedPrecision = 0.9947885690601138\n",
      "Test weightedPrecision = 0.9934785942232548\n",
      "Train weightedRecall = 0.994486634551594\n",
      "Test weightedRecall = 0.9934157584971533\n",
      "Train accuracy = 994486634551594\n",
      "Test accuracy = 0.9934157584971532\n"
     ]
    }
   ],
   "source": [
    "metrics = ['weightedPrecision', 'weightedRecall', 'accuracy']\n",
    "for metric in metrics:\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName=metric)\n",
    "    print('Train ' + metric + ' = ' + str(evaluator.evaluate(train_pred_labels)))\n",
    "    print('Test ' + metric + ' = ' + str(evaluator.evaluate(test_pred_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
